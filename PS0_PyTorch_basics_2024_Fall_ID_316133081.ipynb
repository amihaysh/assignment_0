{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3aXk1o_qRLu"
      },
      "source": [
        "# PyTorch - the Basics!\n",
        "\n",
        "Advanced Learning 2024\n",
        "\n",
        "Lee Carlin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O88JcDDEeVKl"
      },
      "source": [
        "For SUBMISSION:\n",
        "~~~\n",
        "STUDENT ID: 316133081\n",
        "~~~\n",
        "~~~\n",
        "STUDENT GIT LINK: https://github.com/amihaysh/assignment_1.git\n",
        "~~~\n",
        "In Addition, don't forget to add your ID to the files:\n",
        "`PS0_PyTorch_basics_2024_ID_[000000000].ipynb`  \n",
        "`PS0_PyTorch_basics_2024_ID_[000000000].html`  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCByaAJ1q-Gc"
      },
      "source": [
        "## 1. What is PyTorch?\n",
        "\n",
        "\n",
        "PyTorch is a popular open-source library for machine learning, particularly well-suited for deep learning applications.    \n",
        "Here's a breakdown of its key features:\n",
        "\n",
        "    \n",
        "\n",
        "*   **Deep Learning Framework:**  PyTorch provides tools and functionalities to  build and train complex neural networks.\n",
        "*   **Pythonic Interface:**  Known for its Python-like syntax, PyTorch is considered user-friendly and easy to learn.\n",
        "* **Flexibility:**  PyTorch offers both dynamic computational graphs (eager execution) and static graphs (graph mode) for model development.\n",
        "* **Production Ready:**  PyTorch provides features like TorchScript to transition models from development to production seamlessly.\n",
        "* **Scalability:**  PyTorch supports distributed training, enabling you to leverage multiple CPUs or GPUs to train models faster.\n",
        "* **Rich Ecosystem:**  A growing ecosystem of libraries and tools built on PyTorch expands its capabilities for tasks like computer vision, natural language processing, and model interpretability.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0NFG56arhsD"
      },
      "source": [
        "We start by importing PyTorch's main objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "phCzuWkUqPRk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL-emQP0xtkr"
      },
      "source": [
        "### PyTorch Main Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKs60PcGucH7"
      },
      "source": [
        "`torch.nn`:\n",
        "\n",
        "In PyTorch, the `torch.nn` module provides the essential building blocks you need to construct and train neural networks. It offers a comprehensive collection of classes and functions that streamline the deep learning development process.  \n",
        "  \n",
        "*Key Components:*  \n",
        "\n",
        "**Modules**:   \n",
        "These are the fundamental units that perform specific operations on data. They can be combined to create complex neural network architectures. Common examples include:   \n",
        "\n",
        "* Linear Layers (nn.Linear): Apply linear transformations (y = xA^T + b) for feeding data forward through the network.\n",
        "* Convolutional Layers (nn.Conv2d): Perform convolutions, especially useful for processing image data.\n",
        "* Activation Layers (nn.ReLU, nn.Sigmoid): Introduce non-linearity into the network, allowing it to learn complex patterns.\n",
        "* Normalization Layers (nn.BatchNorm2d): Normalize inputs to layers for faster and more stable training.\n",
        "* Recurrent Layers (nn.LSTM, nn.GRU): Handle sequential data like text or time series.\n",
        "* Dropout Layers (nn.Dropout): Introduce randomness by randomly dropping out neurons during training to prevent overfitting.\n",
        "* Many More: PyTorch offers a vast selection of modules catering to diverse neural network architectures.\n",
        "\n",
        "**Containers**:   \n",
        "These classes help you organize and structure your modules into hierarchical networks. They include:\n",
        "* nn.Sequential: Stacks modules in a linear sequence, making it easy to define simple neural networks.\n",
        "* nn.ModuleList: Holds other modules in a list, allowing for more flexible network structures.\n",
        "* nn.ModuleDict: Manages sub-modules with dictionary-like access for complex topologies.\n",
        "\n",
        "**Loss Functions:**   \n",
        "Functions that measure the error between the network's predictions and the ground truth labels. These guide the training process by calculating the gradients used to update the network's weights. Common examples include:\n",
        "* nn.CrossEntropyLoss: For multi-class classification problems.\n",
        "* nn.MSELoss: For mean squared error calculations in regression tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tw5uR0na0s8J"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzHi6U5zvteu"
      },
      "source": [
        "`torch.utils.data.DataLoader`:\n",
        "\n",
        "In PyTorch, `torch.utils.data.DataLoader` is a powerful tool that simplifies how you load and manage data for training your deep learning models. It acts as an iterator, efficiently providing batches of data during the training process.\n",
        "\n",
        "Here's a breakdown of its key functionalities:\n",
        "\n",
        "Data Management Abstraction:\n",
        "\n",
        "    Decouples data loading logic from your model training code, promoting cleaner and more maintainable code.\n",
        "    Handles complexities like batching, shuffling, and multi-processing data loading, freeing you from writing repetitive code.\n",
        "\n",
        "Efficient Batching:\n",
        "\n",
        "    Groups data samples (images, text, etc.) into batches of a specified size (batch_size). Batching improves computational efficiency by utilizing vectorized operations on GPUs.\n",
        "    Provides an optional collate_fn argument that allows you to customize how samples within a batch are combined. This can be useful for tasks like padding sequences to have the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sbHK6GkR0vIH",
        "outputId": "a8496cde-45a7-4833-8191-8ffb1a696755"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.5.0+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sut-qO6F0x1X"
      },
      "source": [
        "In our course we use PyTorch version >=2.2.1.   \n",
        "`cu121` refers to CUDA 12.1, a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements for the execution of compute kernels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE9DKe-9s2aH"
      },
      "source": [
        "## 2. Tensors\n",
        "\n",
        "In PyTorch, tensors are the fundamental data structure. They are similar to NumPy arrays but with some key advantages for deep learning:\n",
        "\n",
        "* **Multi-dimensional Arrays**: Like NumPy arrays, tensors can have multiple dimensions, making them suitable for representing data like images (2D), videos (3D), or sequences of text (1D).\n",
        "\n",
        "* **Hardware Acceleration**: PyTorch tensors can be moved to and run on GPUs or other hardware accelerators, significantly speeding up computations compared to CPUs for deep learning tasks.\n",
        "\n",
        "* **Automatic Differentiation**:  A core feature in deep learning, automatic differentiation allows PyTorch to calculate gradients efficiently, which is essential for training neural networks.  Regular NumPy arrays don't inherently support this.\n",
        "\n",
        "* **Rich Functionality**: PyTorch offers a variety of operations specifically designed for tensors, making it convenient to manipulate and analyze data for deep learning models.\n",
        "\n",
        "In essence, tensors in PyTorch act as the workhorses for your deep learning models. They store and process the data that gets fed into your network, undergoes computations, and ultimately leads to predictions or outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrthmFbIfsXR"
      },
      "source": [
        "**PyTorch tensor operations:**\n",
        "\n",
        "PyTorch tensor operations are the fundamental building blocks for working with data in your deep learning models. These operations allow you to manipulate, analyze, and transform tensors in various ways. Here's a breakdown of some common categories:\n",
        "\n",
        "1. Arithmetic Operations:\n",
        "\n",
        "These operations perform element-wise calculations between tensors or a tensor and a scalar value. They include:\n",
        "\n",
        "    Addition (+)\n",
        "    Subtraction (-)\n",
        "    Multiplication (*)\n",
        "    Division (/)\n",
        "    Exponentiation (**)\n",
        "\n",
        "These operations can be used for simple calculations or combined to create more complex expressions.\n",
        "\n",
        "2. Comparison Operations:\n",
        "\n",
        "These operations compare elements between tensors or a tensor and a scalar value, resulting in a tensor of booleans (True or False) indicating the comparison outcome. Examples include:\n",
        "\n",
        "    Equal (==)\n",
        "    Not equal (!=)\n",
        "    Greater than (>)\n",
        "    Less than (<)\n",
        "    Greater than or equal (>=)\n",
        "    Less than or equal (<=)\n",
        "\n",
        "Comparison operations are useful for filtering data or making decisions within your model.\n",
        "\n",
        "3. Broadcasting:\n",
        "\n",
        "A powerful feature in PyTorch, broadcasting allows operations between tensors of different shapes as long as they are compatible. For instance, you can add a scalar value to a tensor, or add a one-dimensional tensor to a two-dimensional tensor (as long as the dimensions match). PyTorch automatically expands the smaller tensor to match the larger one for element-wise operations.\n",
        "\n",
        "4. In-place Operations:\n",
        "\n",
        "Certain operations modify the original tensor they are applied to, denoted by a trailing underscore (_). Examples include:\n",
        "\n",
        "    x.add_(y) (equivalent to x = x + y)\n",
        "    x.sub_(y) (equivalent to x = x - y)\n",
        "    x.mul_(y) (equivalent to x = x * y)\n",
        "\n",
        "These operations can be memory-efficient when modifying existing tensors is desired.\n",
        "\n",
        "5. Linear Algebra Operations:\n",
        "\n",
        "PyTorch provides functions for common linear algebra operations on tensors, including:\n",
        "\n",
        "    torch.matmul(a, b): Matrix multiplication between tensors a and b.\n",
        "    torch.sum(input, dim=None): Sums the elements of a tensor along a specified dimension.\n",
        "    torch.mean(input, dim=None): Computes the mean of the elements of a tensor along a specified dimension.\n",
        "\n",
        "These operations are essential for various deep learning tasks like calculating activation outputs or loss functions.\n",
        "\n",
        "6. Tensor Reshaping and Indexing:\n",
        "\n",
        "PyTorch offers functionalities to manipulate the shape and access specific elements of tensors:\n",
        "\n",
        "    x.view(new_shape): Reshapes the tensor x into a new shape while keeping the total number of elements the same.\n",
        "    x[index]: Accesses specific elements or sub-tensors using indexing syntax (similar to NumPy).\n",
        "\n",
        "Reshaping and indexing are crucial for preparing data for specific layers in your neural network architecture.\n",
        "\n",
        "7. Element-wise Operations:\n",
        "\n",
        "These operations apply a function to each element of a tensor independently. PyTorch provides a rich set of element-wise functions like:\n",
        "\n",
        "    torch.relu(x): Applies the rectified linear unit (ReLU) activation function.\n",
        "    torch.sigmoid(x): Applies the sigmoid activation function.\n",
        "    torch.tanh(x): Applies the hyperbolic tangent (tanh) activation function.\n",
        "\n",
        "Element-wise operations are fundamental for introducing non-linearity and transforming data in deep learning models.\n",
        "\n",
        "8. Random Operations:\n",
        "\n",
        "PyTorch offers functions for generating random tensors or modifying existing ones with randomness:\n",
        "\n",
        "    torch.rand(shape): Generates a random tensor filled with uniformly distributed values between 0 and 1.\n",
        "    torch.randn(shape): Generates a random tensor filled with values from a standard normal distribution.\n",
        "\n",
        "These operations are useful for data augmentation techniques or initializing weights in your network.\n",
        "\n",
        "By understanding and effectively using these PyTorch tensor operations, you can build and manipulate your deep learning models with greater flexibility and control."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9nYSKhmoAQy"
      },
      "source": [
        "Some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMm7DspXrWpj",
        "outputId": "375296bd-913c-466e-e7bf-36a11614c2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tens: tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "Shape of tensor: torch.Size([2, 4])\n",
            "Datatype of tensor: torch.int64\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ],
      "source": [
        "lst_of_lsts = [[1, 2, 3, 4],[5, 6, 7, 8]]\n",
        "tens = torch.tensor(lst_of_lsts)\n",
        "print(f\"tens: {tens}\")\n",
        "print(f\"Shape of tensor: {tens.shape}\")\n",
        "print(f\"Datatype of tensor: {tens.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tens.device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLgpeDPeoDpG",
        "outputId": "c0d1fd26-e0ab-44e7-b754-ac9b08b2b71e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[[0.8560, 0.8388, 0.7360],\n",
            "         [0.8036, 0.3159, 0.3066]],\n",
            "\n",
            "        [[0.2888, 0.1105, 0.4541],\n",
            "         [0.1843, 0.0681, 0.5922]]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[[1., 1., 1.],\n",
            "         [1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1.],\n",
            "         [1., 1., 1.]]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[[0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.]]])\n"
          ]
        }
      ],
      "source": [
        "shape = (2,2,3)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ADu57CsoykL",
        "outputId": "dc427c04-83e3-44d4-f3cf-cfd6bd527e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t4.shape: torch.Size([300, 3])\n",
            "t5.shape: torch.Size([100, 9])\n"
          ]
        }
      ],
      "source": [
        "shpe = (100,3)\n",
        "t1 = torch.rand(shpe)\n",
        "t2 = torch.rand(shpe)\n",
        "t3 = torch.rand(shpe)\n",
        "t4=torch.cat([t1,t2,t3],dim=0)\n",
        "print(f\"t4.shape: {t4.shape}\")\n",
        "t5=torch.cat([t1,t2,t3],dim=1)\n",
        "print(f\"t5.shape: {t5.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwV4Fnweo1OT",
        "outputId": "3ed7228e-f0f7-4437-c589-88c947ad0bd2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 8.6487,  0.4137,  0.2864,  ..., -1.2689, -0.2099, -1.4781],\n",
              "        [ 0.4137,  9.2644,  0.8763,  ...,  0.0981, -0.4607,  0.7627],\n",
              "        [ 0.2864,  0.8763,  8.0859,  ..., -0.5915, -1.0245,  0.4344],\n",
              "        ...,\n",
              "        [-1.2689,  0.0981, -0.5915,  ...,  8.0523, -0.7799, -0.6304],\n",
              "        [-0.2099, -0.4607, -1.0245,  ..., -0.7799,  8.6987,  0.5653],\n",
              "        [-1.4781,  0.7627,  0.4344,  ..., -0.6304,  0.5653,  7.9003]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "X = torch.rand(100,100)\n",
        "X_cent = X - X.mean(dim=1, keepdim=True)\n",
        "covX = X_cent.T@X_cent\n",
        "covX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qKr9lixtGgO"
      },
      "source": [
        "## Data & Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TJNXJdfuHxH"
      },
      "source": [
        "### PyTorch Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZEnT7gHzWUp"
      },
      "source": [
        "Deep networks are versatile tools that can be adapted to various data types by\n",
        "leveraging appropriate pre-processing techniques and network architectures.\n",
        "PyTorch, like other deep learning libraries, can handle a wide array of data:\n",
        "  \n",
        "\n",
        "\n",
        "* images\n",
        "* audio\n",
        "* text data\n",
        "* tabluar (numerical, categorical, mixed)\n",
        "* multimodal Data\n",
        "* other\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbZ5lmtzuKlb"
      },
      "source": [
        "PyTorch also offers built-in vision specific datasets as part of the `torchvision.datasets` [module](https://pytorch.org/vision/stable/datasets.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9kGSr-D-y42B"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML5of6dVzEAY",
        "outputId": "d17a8e9b-2557-454d-b017-b7ca226b634e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CIFAR10',\n",
              " 'CIFAR100',\n",
              " 'CLEVRClassification',\n",
              " 'CREStereo',\n",
              " 'Caltech101',\n",
              " 'Caltech256',\n",
              " 'CarlaStereo',\n",
              " 'CelebA',\n",
              " 'Cityscapes',\n",
              " 'CocoCaptions',\n",
              " 'CocoDetection',\n",
              " 'Country211',\n",
              " 'DTD',\n",
              " 'DatasetFolder',\n",
              " 'EMNIST',\n",
              " 'ETH3DStereo',\n",
              " 'EuroSAT',\n",
              " 'FER2013',\n",
              " 'FGVCAircraft',\n",
              " 'FakeData',\n",
              " 'FallingThingsStereo',\n",
              " 'FashionMNIST',\n",
              " 'Flickr30k',\n",
              " 'Flickr8k',\n",
              " 'Flowers102',\n",
              " 'FlyingChairs',\n",
              " 'FlyingThings3D',\n",
              " 'Food101',\n",
              " 'GTSRB',\n",
              " 'HD1K',\n",
              " 'HMDB51',\n",
              " 'INaturalist',\n",
              " 'ImageFolder',\n",
              " 'ImageNet',\n",
              " 'Imagenette',\n",
              " 'InStereo2k',\n",
              " 'KMNIST',\n",
              " 'Kinetics',\n",
              " 'Kitti',\n",
              " 'Kitti2012Stereo',\n",
              " 'Kitti2015Stereo',\n",
              " 'KittiFlow',\n",
              " 'LFWPairs',\n",
              " 'LFWPeople',\n",
              " 'LSUN',\n",
              " 'LSUNClass',\n",
              " 'MNIST',\n",
              " 'Middlebury2014Stereo',\n",
              " 'MovingMNIST',\n",
              " 'Omniglot',\n",
              " 'OxfordIIITPet',\n",
              " 'PCAM',\n",
              " 'PhotoTour',\n",
              " 'Places365',\n",
              " 'QMNIST',\n",
              " 'RenderedSST2',\n",
              " 'SBDataset',\n",
              " 'SBU',\n",
              " 'SEMEION',\n",
              " 'STL10',\n",
              " 'SUN397',\n",
              " 'SVHN',\n",
              " 'SceneFlowStereo',\n",
              " 'Sintel',\n",
              " 'SintelStereo',\n",
              " 'StanfordCars',\n",
              " 'UCF101',\n",
              " 'USPS',\n",
              " 'VOCDetection',\n",
              " 'VOCSegmentation',\n",
              " 'VisionDataset',\n",
              " 'WIDERFace',\n",
              " '__all__',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__getattr__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_optical_flow',\n",
              " '_stereo_matching',\n",
              " 'caltech',\n",
              " 'celeba',\n",
              " 'cifar',\n",
              " 'cityscapes',\n",
              " 'clevr',\n",
              " 'coco',\n",
              " 'country211',\n",
              " 'dtd',\n",
              " 'eurosat',\n",
              " 'fakedata',\n",
              " 'fer2013',\n",
              " 'fgvc_aircraft',\n",
              " 'flickr',\n",
              " 'flowers102',\n",
              " 'folder',\n",
              " 'food101',\n",
              " 'gtsrb',\n",
              " 'hmdb51',\n",
              " 'imagenet',\n",
              " 'imagenette',\n",
              " 'inaturalist',\n",
              " 'kinetics',\n",
              " 'kitti',\n",
              " 'lfw',\n",
              " 'lsun',\n",
              " 'mnist',\n",
              " 'moving_mnist',\n",
              " 'omniglot',\n",
              " 'oxford_iiit_pet',\n",
              " 'pcam',\n",
              " 'phototour',\n",
              " 'places365',\n",
              " 'rendered_sst2',\n",
              " 'sbd',\n",
              " 'sbu',\n",
              " 'semeion',\n",
              " 'stanford_cars',\n",
              " 'stl10',\n",
              " 'sun397',\n",
              " 'svhn',\n",
              " 'ucf101',\n",
              " 'usps',\n",
              " 'utils',\n",
              " 'video_utils',\n",
              " 'vision',\n",
              " 'voc',\n",
              " 'widerface']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "dir(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CCQWfKR0etj"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQjlYm3I3bIj",
        "outputId": "83d340a1-eb5a-46aa-9ad6-08e400300463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 202kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.69MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 5.47MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset FashionMNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from torchvision.transforms import ToTensor\n",
        "train_source = training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "train_source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "uq7YInbH3z4K",
        "outputId": "5fdd4cc9-0015-485d-9cb6-a55f7aa41f21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchvision.datasets.mnist.FashionMNIST"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torchvision.datasets.mnist.FashionMNIST</b><br/>def __init__(root: Union[str, Path], train: bool=True, transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, download: bool=False) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py</a>`Fashion-MNIST &lt;https://github.com/zalandoresearch/fashion-mnist&gt;`_ Dataset.\n",
              "\n",
              "Args:\n",
              "    root (str or ``pathlib.Path``): Root directory of dataset where ``FashionMNIST/raw/train-images-idx3-ubyte``\n",
              "        and  ``FashionMNIST/raw/t10k-images-idx3-ubyte`` exist.\n",
              "    train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``,\n",
              "        otherwise from ``t10k-images-idx3-ubyte``.\n",
              "    download (bool, optional): If True, downloads the dataset from the internet and\n",
              "        puts it in root directory. If dataset is already downloaded, it is not\n",
              "        downloaded again.\n",
              "    transform (callable, optional): A function/transform that  takes in a PIL image\n",
              "        and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
              "    target_transform (callable, optional): A function/transform that takes in the\n",
              "        target and transforms it.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 203);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "type(train_source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI0VZXWe5oL_",
        "outputId": "34daed48-391f-48dd-8971-ca4473490b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the shape of the data: torch.Size([60000, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "print(f\"the shape of the data: {train_source.data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvCx0iKV5uPl"
      },
      "source": [
        "We can visualize the data using matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UpM3phXL41eL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "UGVwuqTP4w6B",
        "outputId": "13548030-9917-4600-b9a8-9f4ca48b68b8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 5, 2\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_source), size=(1,)).item()\n",
        "    img, label = train_source[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUArfaqcuF9p"
      },
      "source": [
        "### DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptoV6URJtMyg"
      },
      "source": [
        "As mentioned above, the `torch.utils.data.DataLoader` is responsible for handling data during the deep learning model training.\n",
        "\n",
        "Key Parameters:\n",
        "\n",
        "* dataset (Dataset): The PyTorch dataset you want to load data from (your custom class representing the data and how to access samples and labels).\n",
        "* batch_size (int, optional): The number of samples in a batch (default: 1).\n",
        "* shuffle (bool, optional): Whether to shuffle the data at the beginning of each epoch (default: False).\n",
        "* sampler (Sampler, optional): A custom sampler object for controlling data loading order (default: None).\n",
        "* collate_fn (callable, optional): A function to customize how samples within a batch are combined (default: None). Useful for padding sequences.\n",
        "* pin_memory=True (bool, optional): If using a GPU, pin fetched data tensors in pinned memory for faster transfer (default: False)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "4XyudTrorICU",
        "outputId": "6faa56ff-24d6-4aa5-e786-172ac98817cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.utils.data.dataloader.DataLoader</b><br/>def __init__(dataset: Dataset[_T_co], batch_size: Optional[int]=1, shuffle: Optional[bool]=None, sampler: Union[Sampler, Iterable, None]=None, batch_sampler: Union[Sampler[List], Iterable[List], None]=None, num_workers: int=0, collate_fn: Optional[_collate_fn_t]=None, pin_memory: bool=False, drop_last: bool=False, timeout: float=0, worker_init_fn: Optional[_worker_init_fn_t]=None, multiprocessing_context=None, generator=None, *, prefetch_factor: Optional[int]=None, persistent_workers: bool=False, pin_memory_device: str=&#x27;&#x27;)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py</a>Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
              "\n",
              "The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
              "iterable-style datasets with single- or multi-process loading, customizing\n",
              "loading order and optional automatic batching (collation) and memory pinning.\n",
              "\n",
              "See :py:mod:`torch.utils.data` documentation page for more details.\n",
              "\n",
              "Args:\n",
              "    dataset (Dataset): dataset from which to load the data.\n",
              "    batch_size (int, optional): how many samples per batch to load\n",
              "        (default: ``1``).\n",
              "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
              "        at every epoch (default: ``False``).\n",
              "    sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
              "        samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
              "        implemented. If specified, :attr:`shuffle` must not be specified.\n",
              "    batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
              "        returns a batch of indices at a time. Mutually exclusive with\n",
              "        :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
              "        and :attr:`drop_last`.\n",
              "    num_workers (int, optional): how many subprocesses to use for data\n",
              "        loading. ``0`` means that the data will be loaded in the main process.\n",
              "        (default: ``0``)\n",
              "    collate_fn (Callable, optional): merges a list of samples to form a\n",
              "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
              "        map-style dataset.\n",
              "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
              "        into device/CUDA pinned memory before returning them.  If your data elements\n",
              "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
              "        see the example below.\n",
              "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
              "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
              "        the size of dataset is not divisible by the batch size, then the last batch\n",
              "        will be smaller. (default: ``False``)\n",
              "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
              "        from workers. Should always be non-negative. (default: ``0``)\n",
              "    worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n",
              "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
              "        input, after seeding and before data loading. (default: ``None``)\n",
              "    multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If\n",
              "        ``None``, the default `multiprocessing context`_ of your operating system will\n",
              "        be used. (default: ``None``)\n",
              "    generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
              "        by RandomSampler to generate random indexes and multiprocessing to generate\n",
              "        ``base_seed`` for workers. (default: ``None``)\n",
              "    prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n",
              "        in advance by each worker. ``2`` means there will be a total of\n",
              "        2 * num_workers batches prefetched across all workers. (default value depends\n",
              "        on the set value for num_workers. If value of num_workers=0 default is ``None``.\n",
              "        Otherwise, if value of ``num_workers &gt; 0`` default is ``2``).\n",
              "    persistent_workers (bool, optional): If ``True``, the data loader will not shut down\n",
              "        the worker processes after a dataset has been consumed once. This allows to\n",
              "        maintain the workers `Dataset` instances alive. (default: ``False``)\n",
              "    pin_memory_device (str, optional): the device to :attr:`pin_memory` to if ``pin_memory`` is\n",
              "        ``True``.\n",
              "\n",
              "\n",
              ".. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
              "             cannot be an unpicklable object, e.g., a lambda function. See\n",
              "             :ref:`multiprocessing-best-practices` on more details related\n",
              "             to multiprocessing in PyTorch.\n",
              "\n",
              ".. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
              "             When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
              "             it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
              "             rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
              "             configurations. This represents the best guess PyTorch can make because PyTorch\n",
              "             trusts user :attr:`dataset` code in correctly handling multi-process\n",
              "             loading to avoid duplicate data.\n",
              "\n",
              "             However, if sharding results in multiple workers having incomplete last batches,\n",
              "             this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
              "             be broken into multiple ones and (2) more than one batch worth of samples can be\n",
              "             dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
              "             cases in general.\n",
              "\n",
              "             See `Dataset Types`_ for more details on these two types of datasets and how\n",
              "             :class:`~torch.utils.data.IterableDataset` interacts with\n",
              "             `Multi-process data loading`_.\n",
              "\n",
              ".. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
              "             :ref:`data-loading-randomness` notes for random seed related questions.\n",
              "\n",
              ".. _multiprocessing context:\n",
              "    https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 130);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "train = DataLoader(train_source, batch_size=64, shuffle=True)\n",
        "type(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdKFXjkH6NPg"
      },
      "source": [
        "\n",
        "\n",
        "> The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBYbBhat6DBf",
        "outputId": "38a577be-eeef-498d-be87-6764f1fbcfbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch shape: torch.Size([64, 1, 28, 28])\n",
            "batch shape: torch.Size([64, 1, 28, 28])\n",
            "batch shape: torch.Size([64, 1, 28, 28])\n",
            "batch shape: torch.Size([64, 1, 28, 28])\n",
            "batch shape: torch.Size([64, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  btch = next(iter(train))\n",
        "  print(f\"batch shape: {btch[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOok4gOvaBuW"
      },
      "source": [
        "## Practice Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Fby9XPaJjR"
      },
      "source": [
        "Produce the following tensors:  \n",
        "\n",
        "1. Create a tensor from the list `[1, 2, 3]`\n",
        "2. Create a tensor from a NumPy array of shape (50,10)\n",
        "3. Specify a data type and device option for a tensor with `[7, 8, 9]`, `[.5,0,.7]`\n",
        "4. Create a zero tensor of size (3, 4)\n",
        "5. Create a ones tensor of size (2, 2, 2) with dtype float\n",
        "6. Create a tensor of size (5, 5) with random values from a normal distribution\n",
        "7. Create a new tensor by cloning an existing tensor\n",
        "8. Create a new tensor by reshaping an existing tensor\n",
        "9. Create a new tensor by concatenating two existing tensors\n",
        "10.  Perform operations with different data types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ASqrBB6KeImk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52bbd9f-8f5a-4756-d638-f06ad1905171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([[0.2159, 0.7976, 0.3601, 0.6594, 0.5764, 0.8590, 0.2058, 0.6240, 0.6949,\n",
            "         0.7365],\n",
            "        [0.5647, 0.8825, 0.6776, 0.7755, 0.8403, 0.7976, 0.1029, 0.3738, 0.3651,\n",
            "         0.8124],\n",
            "        [0.8008, 0.9184, 0.6838, 0.8589, 0.2416, 0.1348, 0.4783, 0.6870, 0.1693,\n",
            "         0.0395],\n",
            "        [0.9643, 0.0277, 0.8017, 0.7995, 0.6624, 0.3226, 0.2206, 0.5391, 0.6127,\n",
            "         0.4243],\n",
            "        [0.2516, 0.9852, 0.1335, 0.5399, 0.4029, 0.4137, 0.1001, 0.2903, 0.5229,\n",
            "         0.2437],\n",
            "        [0.9501, 0.2892, 0.1385, 0.5175, 0.3126, 0.3554, 0.0327, 0.4469, 0.7996,\n",
            "         0.5141],\n",
            "        [0.8318, 0.0232, 0.7671, 0.3725, 0.5275, 0.5932, 0.4130, 0.0674, 0.6679,\n",
            "         0.3020],\n",
            "        [0.8896, 0.8445, 0.1389, 0.6007, 0.2264, 0.4166, 0.2434, 0.2771, 0.8830,\n",
            "         0.8717],\n",
            "        [0.0959, 0.5815, 0.9831, 0.0316, 0.5509, 0.8841, 0.9690, 0.3801, 0.0050,\n",
            "         0.0304],\n",
            "        [0.0889, 0.4986, 0.7731, 0.9471, 0.9747, 0.3263, 0.3284, 0.3658, 0.3737,\n",
            "         0.2488],\n",
            "        [0.2206, 0.8013, 0.0793, 0.3699, 0.0091, 0.3638, 0.9374, 0.0711, 0.3300,\n",
            "         0.2247],\n",
            "        [0.1575, 0.7349, 0.2683, 0.2796, 0.8319, 0.3358, 0.8872, 0.0147, 0.1178,\n",
            "         0.7851],\n",
            "        [0.2098, 0.5828, 0.2731, 0.1619, 0.1179, 0.1338, 0.0475, 0.3578, 0.9381,\n",
            "         0.2587],\n",
            "        [0.7627, 0.5297, 0.2282, 0.6864, 0.0052, 0.4479, 0.2788, 0.8159, 0.0713,\n",
            "         0.8314],\n",
            "        [0.2194, 0.1822, 0.7803, 0.2771, 0.2879, 0.2221, 0.8810, 0.5554, 0.9660,\n",
            "         0.5757],\n",
            "        [0.9027, 0.9497, 0.9023, 0.5925, 0.2834, 0.3430, 0.2379, 0.2779, 0.9601,\n",
            "         0.8752],\n",
            "        [0.0140, 0.8959, 0.8097, 0.2748, 0.5076, 0.7242, 0.7836, 0.6783, 0.5601,\n",
            "         0.5563],\n",
            "        [0.5167, 0.8616, 0.7124, 0.4505, 0.6957, 0.6611, 0.2697, 0.9450, 0.7909,\n",
            "         0.9219],\n",
            "        [0.5946, 0.7590, 0.7648, 0.3981, 0.5680, 0.2391, 0.7987, 0.2654, 0.3734,\n",
            "         0.8467],\n",
            "        [0.5592, 0.9477, 0.5612, 0.4415, 0.4700, 0.1568, 0.4236, 0.7492, 0.4708,\n",
            "         0.8834],\n",
            "        [0.9779, 0.7702, 0.8393, 0.8159, 0.0923, 0.2322, 0.4999, 0.3960, 0.7402,\n",
            "         0.5708],\n",
            "        [0.2219, 0.8757, 0.4795, 0.1892, 0.9647, 0.3701, 0.0142, 0.0511, 0.5223,\n",
            "         0.9154],\n",
            "        [0.3220, 0.4797, 0.6604, 0.3280, 0.7185, 0.3322, 0.5652, 0.5797, 0.9237,\n",
            "         0.5459],\n",
            "        [0.4602, 0.8394, 0.9370, 0.4036, 0.5521, 0.3011, 0.7641, 0.7499, 0.2600,\n",
            "         0.3677],\n",
            "        [0.4010, 0.0749, 0.3059, 0.2927, 0.5283, 0.7574, 0.4900, 0.9857, 0.2958,\n",
            "         0.1564],\n",
            "        [0.1649, 0.8387, 0.9091, 0.2889, 0.4575, 0.3663, 0.8207, 0.9104, 0.4030,\n",
            "         0.8474],\n",
            "        [0.4324, 0.1448, 0.8993, 0.3333, 0.8348, 0.8096, 0.0279, 0.0988, 0.9344,\n",
            "         0.8577],\n",
            "        [0.5230, 0.8928, 0.3790, 0.8395, 0.7739, 0.8803, 0.6713, 0.5706, 0.6505,\n",
            "         0.0720],\n",
            "        [0.0205, 0.6595, 0.4422, 0.5148, 0.6020, 0.3190, 0.0871, 0.0316, 0.8588,\n",
            "         0.9650],\n",
            "        [0.0049, 0.2904, 0.2455, 0.7340, 0.2124, 0.5723, 0.5622, 0.5950, 0.7986,\n",
            "         0.6212],\n",
            "        [0.5014, 0.7215, 0.9106, 0.6333, 0.2911, 0.3374, 0.1401, 0.2752, 0.4261,\n",
            "         0.7838],\n",
            "        [0.1712, 0.4834, 0.9845, 0.3140, 0.5984, 0.1907, 0.4106, 0.8423, 0.7333,\n",
            "         0.3457],\n",
            "        [0.3412, 0.8711, 0.0440, 0.6449, 0.0916, 0.1692, 0.5678, 0.6300, 0.1597,\n",
            "         0.0497],\n",
            "        [0.5966, 0.4091, 0.8116, 0.8519, 0.8421, 0.5087, 0.4172, 0.7078, 0.0686,\n",
            "         0.4411],\n",
            "        [0.7014, 0.5246, 0.2111, 0.2046, 0.9175, 0.9953, 0.0818, 0.2631, 0.3766,\n",
            "         0.7507],\n",
            "        [0.0366, 0.8099, 0.3946, 0.7858, 0.7760, 0.2326, 0.3207, 0.8977, 0.3139,\n",
            "         0.3334],\n",
            "        [0.7609, 0.6302, 0.4694, 0.8804, 0.7790, 0.6345, 0.9484, 0.4491, 0.2972,\n",
            "         0.4034],\n",
            "        [0.0915, 0.1046, 0.3340, 0.0200, 0.2427, 0.1681, 0.0763, 0.2258, 0.6224,\n",
            "         0.9511],\n",
            "        [0.1417, 0.9838, 0.8145, 0.8931, 0.5177, 0.2888, 0.4985, 0.8324, 0.8806,\n",
            "         0.6931],\n",
            "        [0.6569, 0.0605, 0.5283, 0.8616, 0.5687, 0.8828, 0.2737, 0.1994, 0.2484,\n",
            "         0.0320],\n",
            "        [0.9695, 0.0098, 0.2272, 0.1182, 0.7371, 0.0636, 0.1108, 0.0508, 0.1294,\n",
            "         0.4689],\n",
            "        [0.6840, 0.9446, 0.3046, 0.8872, 0.5192, 0.2386, 0.7066, 0.7377, 0.2422,\n",
            "         0.4221],\n",
            "        [0.7771, 0.0951, 0.6187, 0.2161, 0.1072, 0.9031, 0.5847, 0.3448, 0.7407,\n",
            "         0.8688],\n",
            "        [0.0300, 0.3617, 0.4648, 0.0164, 0.5278, 0.5435, 0.7755, 0.7717, 0.3453,\n",
            "         0.8104],\n",
            "        [0.8579, 0.2550, 0.0749, 0.1132, 0.9705, 0.8191, 0.0626, 0.7161, 0.2199,\n",
            "         0.5778],\n",
            "        [0.6880, 0.9483, 0.3373, 0.2278, 0.7206, 0.5753, 0.5318, 0.0142, 0.6469,\n",
            "         0.4673],\n",
            "        [0.0865, 0.1396, 0.9796, 0.1039, 0.7143, 0.5825, 0.9391, 0.6684, 0.9655,\n",
            "         0.8991],\n",
            "        [0.6007, 0.3454, 0.6680, 0.7239, 0.4785, 0.9508, 0.6799, 0.1367, 0.8473,\n",
            "         0.1273],\n",
            "        [0.4861, 0.4450, 0.2314, 0.7560, 0.7362, 0.9315, 0.2717, 0.1171, 0.5067,\n",
            "         0.6403],\n",
            "        [0.7099, 0.5703, 0.8570, 0.0503, 0.8344, 0.9387, 0.4524, 0.4381, 0.8527,\n",
            "         0.5583]], dtype=torch.float64)\n",
            "tensor([[7.0000, 8.0000, 9.0000],\n",
            "        [0.5000, 0.0000, 0.7000]])\n",
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "tensor([[[1., 1.],\n",
            "         [1., 1.]],\n",
            "\n",
            "        [[1., 1.],\n",
            "         [1., 1.]]])\n",
            "tensor([[-0.9255,  0.9606, -0.2213, -1.3134,  0.9787],\n",
            "        [ 0.8829, -1.2203, -0.2274,  1.4039,  0.3928],\n",
            "        [-1.2930, -1.1342,  1.7217,  1.1270,  1.5338],\n",
            "        [-0.4693, -0.3837, -0.6092, -1.4485,  0.2569],\n",
            "        [ 1.5765, -0.3036, -0.2788, -0.9261, -0.7431]])\n",
            "tensor([[-0.9255,  0.9606, -0.2213, -1.3134,  0.9787],\n",
            "        [ 0.8829, -1.2203, -0.2274,  1.4039,  0.3928],\n",
            "        [-1.2930, -1.1342,  1.7217,  1.1270,  1.5338],\n",
            "        [-0.4693, -0.3837, -0.6092, -1.4485,  0.2569],\n",
            "        [ 1.5765, -0.3036, -0.2788, -0.9261, -0.7431]])\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "tensor([[-0.9255,  0.9606, -0.2213, -1.3134,  0.9787, -0.9255,  0.9606, -0.2213,\n",
            "         -1.3134,  0.9787],\n",
            "        [ 0.8829, -1.2203, -0.2274,  1.4039,  0.3928,  0.8829, -1.2203, -0.2274,\n",
            "          1.4039,  0.3928],\n",
            "        [-1.2930, -1.1342,  1.7217,  1.1270,  1.5338, -1.2930, -1.1342,  1.7217,\n",
            "          1.1270,  1.5338],\n",
            "        [-0.4693, -0.3837, -0.6092, -1.4485,  0.2569, -0.4693, -0.3837, -0.6092,\n",
            "         -1.4485,  0.2569],\n",
            "        [ 1.5765, -0.3036, -0.2788, -0.9261, -0.7431,  1.5765, -0.3036, -0.2788,\n",
            "         -0.9261, -0.7431]])\n",
            "tensor([[-1.8511,  1.9211, -0.4425, -2.6269,  1.9575],\n",
            "        [ 1.7659, -2.4407, -0.4548,  2.8078,  0.7855],\n",
            "        [-2.5860, -2.2683,  3.4435,  2.2540,  3.0676],\n",
            "        [-0.9385, -0.7673, -1.2184, -2.8971,  0.5139],\n",
            "        [ 3.1530, -0.6071, -0.5576, -1.8521, -1.4863]])\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "#Q1\n",
        "t1=torch.tensor([1,2,3])\n",
        "print(t1)\n",
        "#Q2\n",
        "import numpy as np\n",
        "t2=torch.from_numpy(np.random.rand(50,10))\n",
        "print(t2)\n",
        "#Q3\n",
        "t3=torch.tensor([[7, 8, 9], [0.5,0,0.7]], dtype=torch.float, device='cpu')\n",
        "print(t3)\n",
        "#Q4\n",
        "t4=torch.zeros((3, 4))\n",
        "print(t4)\n",
        "#Q5\n",
        "t5=torch.ones((2, 2, 2), dtype=torch.float)\n",
        "print(t5)\n",
        "#Q6\n",
        "t6=torch.randn((5, 5))\n",
        "print(t6)\n",
        "#Q7\n",
        "t7=t6.clone()\n",
        "print(t7)\n",
        "#Q8\n",
        "t8=t5.reshape(2,4)\n",
        "print(t8)\n",
        "#Q9\n",
        "t9=torch.cat((t6,t7),dim=1)\n",
        "print(t9)\n",
        "#Q10\n",
        "t10=t6+t7\n",
        "print(t10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvL6c_cEboV_"
      },
      "source": [
        "1. Inspect the  attributes of some of the tensors you created with `shape,dtype,numel`.\n",
        "2. Indexing to access specific elements: `[[1, 2, 3], [4, 5, 6]]`, get the '2' element.\n",
        "\n",
        "3. Modify modify the '5' element to '10'\n",
        "4. Apply element-wise addition to two tensors.\n",
        "5. Apply element-wise multiplication to two tensors\n",
        "6. Apply a square root to a tensor.\n",
        "7. Create a tensor with random values from a uniform distribution between 0 and 1.\n",
        "8. Create a tensor with random values from a normal distribution with mean 0 and standard deviation 1\n",
        "9. Create a tensor with random values from a discrete uniform distribution between 1 and 10.\n",
        "10. Reshape a tensor from size `(2, 3)` to `(3, 2)`\n",
        "11. Transpose a tensor\n",
        "12. Concatenate tensors along a specific dimension\n",
        "13. Stack tensors along a new dimension\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "d_eeWx27eLGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381a1b54-b59d-4867-9a0a-c004b515f27f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: torch.Size([3]), Dtype: torch.int64, Number of elements: 3\n",
            "tensor(2)\n",
            "tensor([[ 1,  2,  3],\n",
            "        [ 4, 10,  6]])\n",
            "tensor([[-1.8511,  1.9211, -0.4425, -2.6269,  1.9575],\n",
            "        [ 1.7659, -2.4407, -0.4548,  2.8078,  0.7855],\n",
            "        [-2.5860, -2.2683,  3.4435,  2.2540,  3.0676],\n",
            "        [-0.9385, -0.7673, -1.2184, -2.8971,  0.5139],\n",
            "        [ 3.1530, -0.6071, -0.5576, -1.8521, -1.4863]])\n",
            "tensor([[0.8566, 0.9227, 0.0490, 1.7251, 0.9579],\n",
            "        [0.7796, 1.4892, 0.0517, 1.9710, 0.1543],\n",
            "        [1.6719, 1.2863, 2.9643, 1.2701, 2.3526],\n",
            "        [0.2202, 0.1472, 0.3711, 2.0983, 0.0660],\n",
            "        [2.4854, 0.0921, 0.0777, 0.8576, 0.5523]])\n",
            "tensor([[   nan, 0.9801,    nan,    nan, 0.9893],\n",
            "        [0.9396,    nan,    nan, 1.1849, 0.6267],\n",
            "        [   nan,    nan, 1.3121, 1.0616, 1.2385],\n",
            "        [   nan,    nan,    nan,    nan, 0.5069],\n",
            "        [1.2556,    nan,    nan,    nan,    nan]])\n",
            "tensor([0.5495, 0.4933, 0.2006, 0.4072, 0.1907, 0.2981, 0.0372, 0.1475, 0.0621,\n",
            "        0.6993])\n",
            "tensor([ 0.4943, -1.3184, -0.9766,  1.7268,  0.7236,  0.3624,  0.3201, -0.6393,\n",
            "        -0.2589, -0.4901])\n",
            "tensor([7, 8, 6, 9, 8, 8, 8, 3, 1, 3])\n",
            "tensor([[0.7120, 0.5574, 0.5840],\n",
            "        [0.9976, 0.5272, 0.1190]])\n",
            "tensor([[0.7120, 0.5574],\n",
            "        [0.5840, 0.9976],\n",
            "        [0.5272, 0.1190]])\n",
            "tensor([[0.7120, 0.5840, 0.5272],\n",
            "        [0.5574, 0.9976, 0.1190]])\n",
            "tensor([[-0.9255,  0.9606, -0.2213, -1.3134,  0.9787],\n",
            "        [ 0.8829, -1.2203, -0.2274,  1.4039,  0.3928],\n",
            "        [-1.2930, -1.1342,  1.7217,  1.1270,  1.5338],\n",
            "        [-0.4693, -0.3837, -0.6092, -1.4485,  0.2569],\n",
            "        [ 1.5765, -0.3036, -0.2788, -0.9261, -0.7431],\n",
            "        [-0.9255,  0.9606, -0.2213, -1.3134,  0.9787],\n",
            "        [ 0.8829, -1.2203, -0.2274,  1.4039,  0.3928],\n",
            "        [-1.2930, -1.1342,  1.7217,  1.1270,  1.5338],\n",
            "        [-0.4693, -0.3837, -0.6092, -1.4485,  0.2569],\n",
            "        [ 1.5765, -0.3036, -0.2788, -0.9261, -0.7431]])\n",
            "tensor([[[-0.9255,  0.9606, -0.2213, -1.3134,  0.9787],\n",
            "         [ 0.8829, -1.2203, -0.2274,  1.4039,  0.3928],\n",
            "         [-1.2930, -1.1342,  1.7217,  1.1270,  1.5338],\n",
            "         [-0.4693, -0.3837, -0.6092, -1.4485,  0.2569],\n",
            "         [ 1.5765, -0.3036, -0.2788, -0.9261, -0.7431]],\n",
            "\n",
            "        [[-0.9255,  0.9606, -0.2213, -1.3134,  0.9787],\n",
            "         [ 0.8829, -1.2203, -0.2274,  1.4039,  0.3928],\n",
            "         [-1.2930, -1.1342,  1.7217,  1.1270,  1.5338],\n",
            "         [-0.4693, -0.3837, -0.6092, -1.4485,  0.2569],\n",
            "         [ 1.5765, -0.3036, -0.2788, -0.9261, -0.7431]]])\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "#Q1\n",
        "print(f\"Shape: {t1.shape}, Dtype: {t1.dtype}, Number of elements: {t1.numel()}\")\n",
        "#Q2\n",
        "t2=torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(t2[0,1])\n",
        "#Q3\n",
        "t2[1,1]=10\n",
        "print(t2)\n",
        "#Q4\n",
        "print(t6+t7)\n",
        "#Q5\n",
        "print(t6*t7)\n",
        "#Q6\n",
        "print(torch.sqrt(t6))\n",
        "#Q7\n",
        "t7=torch.rand(10)\n",
        "print(t7)\n",
        "#Q8\n",
        "t8=torch.randn(10)\n",
        "print(t8)\n",
        "#Q9\n",
        "t9=torch.randint(1,10,size=(10,))\n",
        "print(t9)\n",
        "#Q10\n",
        "t10=torch.rand(2,3)\n",
        "print(t10)\n",
        "t10=t10.reshape(3,2)\n",
        "print(t10)\n",
        "#Q11\n",
        "t11=t10.T\n",
        "print(t11)\n",
        "#Q12\n",
        "t12=torch.cat((t6,t6),dim=0)\n",
        "print(t12)\n",
        "#Q13\n",
        "t13=torch.stack((t6,t6),dim=0)\n",
        "print(t13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Ag4aLvd186"
      },
      "source": [
        "1. Sum of all elements in a tensor.\n",
        "2. Mean along a specific dimension\n",
        "3. Find the minimum and maximum values in a tensor\n",
        "4. Find the indices of minimum and maximum values in a tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YS_zcuzPeL2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "220bb0d3-ae38-49e4-82de-c5de7039d7b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.6625)\n",
            "tensor([-0.0457, -0.4162,  0.0770, -0.2314,  0.4838])\n",
            "tensor(-1.4485)\n",
            "tensor(1.7217)\n",
            "tensor(18)\n",
            "tensor(12)\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "#Q1\n",
        "print(t6.sum())\n",
        "#Q2\n",
        "print(t6.mean(dim=0))\n",
        "#Q3\n",
        "print(t6.min())\n",
        "print(t6.max())\n",
        "#Q4\n",
        "print(t6.argmin())\n",
        "print(t6.argmax())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUhF40qoc9lh"
      },
      "source": [
        "Broadcasting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ3jXo9F7YyB",
        "outputId": "ae4d514d-9945-44da-9c68-ec733dc0715e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broadcasting with scalars:\n",
            "tensor([6, 7, 8])\n",
            "Broadcasting with different shapes:\n",
            "tensor([[11, 22, 33],\n",
            "        [14, 25, 36]])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Broadcasting with scalars\n",
        "tensor_a = torch.tensor([1, 2, 3])\n",
        "scalar_b = 5\n",
        "result_broadcast_scalar = tensor_a + scalar_b\n",
        "print(\"Broadcasting with scalars:\")\n",
        "print(result_broadcast_scalar)  # Output: tensor([6, 7, 8])\n",
        "\n",
        "# Broadcasting with different shapes\n",
        "tensor_c = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "tensor_d = torch.tensor([10, 20, 30])\n",
        "result_broadcast_shape = tensor_c + tensor_d\n",
        "print(\"Broadcasting with different shapes:\")\n",
        "print(result_broadcast_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTTx-zG3dFzd",
        "outputId": "31f2ac87-5fe5-454b-d8b0-5bf2ee72aaff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broadcasting with multidimensional tensors:\n",
            "tensor([[11, 22, 33],\n",
            "        [14, 25, 36]])\n",
            "RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1\n"
          ]
        }
      ],
      "source": [
        "# Broadcasting with multidimensional tensors\n",
        "tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "tensor_b = torch.tensor([10, 20, 30])\n",
        "result_broadcast = tensor_a + tensor_b\n",
        "print(\"Broadcasting with multidimensional tensors:\")\n",
        "print(result_broadcast)\n",
        "\n",
        "# Common broadcasting pitfalls\n",
        "tensor_c = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "tensor_d = torch.tensor([10, 20])\n",
        "# The following line will raise a RuntimeError\n",
        "try:\n",
        "    result_pitfall = tensor_c + tensor_d\n",
        "except RuntimeError as e:\n",
        "    print(\"RuntimeError:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8XnR20UdL9x"
      },
      "source": [
        "Device specification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmOPlcDndLei",
        "outputId": "eed27f3a-6846-434e-cda0-fac05fba40e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is GPU available? False\n",
            "Tensor on GPU: tensor([1, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "# Code examples for device configuration and availability\n",
        "\n",
        "# Check if GPU is available\n",
        "is_gpu_available = torch.cuda.is_available()\n",
        "print(\"Is GPU available?\", is_gpu_available)\n",
        "\n",
        "# Specify device for tensor operations\n",
        "device = torch.device('cuda' if is_gpu_available else 'cpu')\n",
        "tensor_gpu = torch.tensor([1, 2, 3], device=device)\n",
        "print(\"Tensor on GPU:\", tensor_gpu)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTb9_VWsgCkv"
      },
      "source": [
        "Dataloader practice:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-XSmZ7Tffq2"
      },
      "source": [
        "Using the Fashion MNIST dataloader object, iterate over the batches, and calculate an interesting statistic. Make sure the statistic represent the entire data, not just a single batch. Hint- find a way to \"update\" the statistic along the batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ED4PArw7e43C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d079e0df-5a9c-4f6e-a93a-f8f01194cdee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean pixel intensity across the entire Fashion MNIST dataset: 0.2860\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "from torchvision import transforms\n",
        "transform = transforms.ToTensor()\n",
        "fashion_mnist = datasets.FashionMNIST(root='data', train=True, transform=transform, download=True)\n",
        "batch_size = 64\n",
        "dataloader = DataLoader(fashion_mnist, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "cumulative_sum = 0.0\n",
        "total_pixels = 0\n",
        "\n",
        "for images, _ in dataloader:\n",
        "    batch_sum = images.sum()\n",
        "    batch_pixels = images.numel()\n",
        "    cumulative_sum += batch_sum.item()\n",
        "    total_pixels += batch_pixels\n",
        "\n",
        "mean_pixel_intensity = cumulative_sum / total_pixels\n",
        "print(f\"Mean pixel intensity across the entire Fashion MNIST dataset: {mean_pixel_intensity:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrhHnCUOXeTY"
      },
      "source": [
        "## How to export an `HTML` file of your `ipynb`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yvoSJjmXqah"
      },
      "source": [
        "1. Save your notebook and make sure all the cells have the expected output.\n",
        "2. Download your notebook to your local machine.   `File-->Download-->Download ipynb`\n",
        "3.  Reupload it so Colab can see it :  Click on the Files icon on the far left bar--> you should see your current kernel folder --> click `upload to session storage` --> upload your file.\n",
        "4. Execute the following: `!jupyter nbconvert --to html /content/NOTEBOOKFILE.ipynb`\n",
        "5. You should see the html file produced in your kernel's current folder. You can download it locally.\n",
        "[link text](https://)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKpF6F5E741a"
      },
      "source": [
        " END OF PyTorch BASICS"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}